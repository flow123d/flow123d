==============================
How to install and run Flow123
==============================

==============
Prerequisities
==============

If you are running Windows, you have to install 'cygwin' for emulation of POSIX unix environment.

For build you will need some developement software packages. On Windows use cygwin to install them. On Linux use package manager
of your distribution ('apt-get' for Ubuntu and Debian, 'yum' for RedHat and Centos)

requested packages are: 

* gcc, g++, gfortran    C/C++ compiler and Fortran77 compiler for compilation of BLAS library.
* python, perl, cmake   Scripting languages and building tool
* boost                 C++ library

optionaly you may want also:

* doxygen, graphviz     source generated documentation and its supprot tool for diagrams.
* eclipse + CDT (C/C++ developement toolkit)   

You can find complete set of usefull libraries compatible with Flow123d on address:

http://dev.nti.tul.cz/jan.brezina/flow_libs (check)

==============================
Step 1 - Install PETSc Library
==============================

Flow branches up to 1.6.5 depends on the PETSC library 3.0.0-xx.
Download this version from:

http://www.mcs.anl.gov/petsc/petsc-as/documentation/installation.html

Assume that you unpack the installation tree to the directory:

/home/jb/local/petsc

Change to this directory and set this as your PETSC directory:
> export PETSC_DIR=`pwd`

For developelemt you will need at least debuging and production build of the library. 
First set a name for the debugging configuration:
> export PETSC_ARCH=linux-gcc-dbg

And run the configuration script, for example with following options:
>./config/configure.py --with-debugging=1 --with-cc=gcc --with-cxx=g++ --with-fc=gfortran --CFLAGS_O=-g --FFLAGS_O=-g --download-mpich=yes --download-parmetis=yes --download-f-blas-lapack=1 --with-x=1

This also automagically install BLAS, Lapack, MPICH, and ParMetis so it takes a while, it can be about 15 min. 
If everything is OK, you obtain table with used compilers and libraries. Finally compile PETSC with this configuration:
> make all

To test the compilation run:
> make test

To obtain PETSC configuration for the production version you can use e.g.
> export PETSC_ARCH=linux-gcc-dbg
>./config/configure.py --with-debugging=0 --with-cc=gcc --with-cxx=g++ --with-fc=gfortran --CFLAGS_O=-O3 --FFLAGS_O=-O3 --download-mpich=yes --download-parmetis=yes --download-f-blas-lapack=1 --with-x=1
> make all
> make test

Configurations mentioned abouve are minimalistic. Next we describe several additional cofigure options whic can be usefull.


Alternatives and Troubleshooting
================================



1) If you want only serial version of PETSc (and Flow123d)
add --with-mpi=0 to the configure command line.
You can have several PETSC configuration using different PETSC_ARCH.

2) By default PETSC will create dynamicaly linked libraries, which can be shared be more applications. But on some systems 
(in particular we have problems under Windows) this doesn't work, so one is forced to turn off dynamic linking by:

--with-shared=0

3) PETSC use BLAS and few LAPCAK functions for its local vector and matrix operations. The speed of BLAS and LAPACK have dramatic impact 
   on the overall performance. There is a sophisicated implementation of BLAS called ATLAS. ATLAS performs extensive set of performance tests on
   your hardware then make an optimized imlementation of  BLAS code for you. Accoring to our measurements the Flow123d is about two times faster
   with ATLAS compared to usual --download-f-blas-lapack (on x86 architecture and usin GCC).
   
   In order to use ATLAS, download it from ... and follow their instructions. The key point is that you have to turn off the CPU throttling.
   To this end install 'cpufreq-set' or `cpu-freq-selector` and use it to set your processor to maximal performance:
   >cpufreq-set -c 0 -g performance
   >cpufreq-set -c 1 -g performance

   ... this way I have set performance mode for both cores of my Core2Duo.

   Then you need not to specify any special options, just run default configuration and make. 
   
   Unfortunately, there is one experimental preconditioner in PETSC (PCASA) which use a QR decomposition Lapack function, that is not
   part of ATLAS. Although it is possible to combine ATLAS with full LAPACK from Netlib, we rather provide an empty QR decomposition function
   as a part of Flow123d sources.
   See. HAVE_ATTLAS_ONLY_LAPACK in ./makefile.in

4) PETSC provides interface to many usefull packages. You can install them 
adding further configure options:
--download-superlu=yes         # parallel direct solver
--download-hypre=yes           # Boomer algebraic multigrid preconditioner, many preconditioners
--download-spools=yes          # parallel direc solver
--download-blacs=ifneeded      # needed by MUMPS
--download-scalapack=ifneeded  # needed by MUMPS
--download-mumps=yes           # parallel direct solver
--download-umfpack=yes         # MATLAB solver

For further information about use of these packages see:

http://www.mcs.anl.gov/petsc/petsc-2/documentation/linearsolvertable.html

http://www.mcs.anl.gov/petsc/petsc-as/snapshots/petsc-current/docs/manualpages/PC/PCFactorSetMatSolverPackage.html#PCFactorSetMatSolverPackage
http://www.mcs.anl.gov/petsc/petsc-as/snapshots/petsc-current/docs/manualpages/Mat/MAT_SOLVER_SPOOLES.html#MAT_SOLVER_SPOOLES
http://www.mcs.anl.gov/petsc/petsc-as/snapshots/petsc-current/docs/manualpages/Mat/MAT_SOLVER_MUMPS.html#MAT_SOLVER_MUMPS
http://www.mcs.anl.gov/petsc/petsc-as/snapshots/petsc-current/docs/manualpages/Mat/MAT_SOLVER_SUPERLU_DIST.html
http://www.mcs.anl.gov/petsc/petsc-as/snapshots/petsc-current/docs/manualpages/Mat/MAT_SOLVER_UMFPACK.html

http://www.mcs.anl.gov/petsc/petsc-as/snapshots/petsc-dev/docs/manualpages/PC/PCHYPRE.html

=========================
 Step 2 - Armadillo
=========================

.. under construction.

=========================
 Step 3 - DUNE
=========================

... under construction

=========================
 Step 4 - Compile Flow123
=========================

Get appropriate branch from repository. "trunk_serial" should be stable, "branches/1.6.0_modular" 
is actual development branch.

Copy file  makefile.in.template to makefile.in:
> cp makefile.in.template makefile.in

Edit file makefile.in, set your PETSC_DIR and PETSC_ARCH.

Uncomment setting for "USE_PARALLEL" if you want a parallel version.
To this end you need PETSC with treu MPI (not --with-mpi=0) and with ParMETIS library.
Alternatively you can install ParMETIS yourself and provide its location in variables:
PARMETIS_INCLUDE, PARMETIS_LIB (as indicated in makefile.in.template)

Finally you can specify the C++ compiler and its options. Otherwise we use PETSC setting.
e.g. for GCC:
CPP=g++
CFLAGS= -O3

Then run the compilation by:
> make all

For further information about program usage see documentation in "doc/" in particular 
reference manual "doc/flow_doc". 




TODO:
- potreba doxygen, graphviz
- pozadavky na PETSC: parmetis, (mumps)
- armadillo
- boost
- cmake


